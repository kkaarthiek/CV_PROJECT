{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7Wn3pRU3mhM"
      },
      "source": [
        "# Fine-Tuning SwinV2 for Real vs. AI-Generated Image Classification\n",
        "\n",
        "This notebook provides a step-by-step guide to training a **SwinV2 Small** model to classify images as either 'real' or 'fake' (AI-generated).\n",
        "\n",
        "The process covers:\n",
        "1.  **Setup**: Mounting Google Drive and installing necessary libraries.\n",
        "2.  **Data Preparation**: Loading your `metadata.json` and splitting the dataset.\n",
        "3.  **Dataset Creation**: Using a custom PyTorch `Dataset` for efficient data loading.\n",
        "4.  **Model Configuration**: Loading the pre-trained SwinV2 model and adapting it for our binary classification task.\n",
        "5.  **Training**: Fine-tuning the model using the Hugging Face `Trainer` API.\n",
        "6.  **Inference**: Using the trained model to make predictions on new images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuYh10_43mhP"
      },
      "source": [
        "### Step 1: Mount Google Drive & Install Libraries\n",
        "\n",
        "First, we connect to Google Drive to access our dataset. Then, we install the required Python packages from Hugging Face and other utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74MES6nx3mhQ",
        "outputId": "2c441f1a-8ba8-4ca1-86a5-226c7b592e7b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install required libraries quietly\n",
        "!pip install -q transformers datasets evaluate accelerate pandas scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBClKJeg3mhR"
      },
      "source": [
        "### Step 2: Prepare the Data\n",
        "\n",
        "Here, we'll load the `metadata.json` file into a pandas DataFrame. We'll construct the full path to each image and convert the 'real'/'fake' status into numerical labels (0/1). Finally, we split the data into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1xI2O_dYV0i",
        "outputId": "edc71f65-844c-44be-d6e6-3c27b9eb2e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading metadata from: ./data\\metadata.json\n",
            "\n",
            "Total samples: 14062\n",
            "Training samples: 11249\n",
            "Validation samples: 2813\n",
            "\n",
            "Sample of the training dataframe:\n",
            "      id                  image_file  \\\n",
            "0  13103  real/hf_unsplash_41569.jpg   \n",
            "1  12568  real/hf_unsplash_28579.jpg   \n",
            "2  10431  real/hf_real_image_261.jpg   \n",
            "3   6411  fake_DALLE/4676717700.webp   \n",
            "4   2607  fake_SD/Images_sd3 432.jpg   \n",
            "\n",
            "                                             prompts platform status  \\\n",
            "0                                               real     None   real   \n",
            "1                                               real     None   real   \n",
            "2                                               real     None   real   \n",
            "3  A drummer is playing while wearing sunglasses,...  dall-E3   fake   \n",
            "4   A man surrounded by a group of people is look...       sd   fake   \n",
            "\n",
            "                           full_path  label  \n",
            "0  ./data\\real/hf_unsplash_41569.jpg      0  \n",
            "1  ./data\\real/hf_unsplash_28579.jpg      0  \n",
            "2  ./data\\real/hf_real_image_261.jpg      0  \n",
            "3  ./data\\fake_DALLE/4676717700.webp      1  \n",
            "4  ./data\\fake_SD/Images_sd3 432.jpg      1  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Configuration ---\n",
        "# â¬‡ï¸ IMPORTANT: Change this path to where your 'data' folder is located in Google Drive\n",
        "BASE_DIR = \"./data\"\n",
        "METADATA_FILE = os.path.join(BASE_DIR, \"metadata.json\")\n",
        "\n",
        "# --- Load and Process Metadata ---\n",
        "print(f\"Loading metadata from: {METADATA_FILE}\")\n",
        "df = pd.read_json(METADATA_FILE)\n",
        "\n",
        "# The 'image_file' column might be a list, let's flatten it if so\n",
        "# This handles cases where metadata.json has entries like [[\"real/image1.png\"]]\n",
        "if isinstance(df['image_file'].iloc[0], list):\n",
        "    df['image_file'] = df['image_file'].str[0]\n",
        "\n",
        "# Create the full, absolute path for each image file\n",
        "df['full_path'] = df['image_file'].apply(lambda x: os.path.join(BASE_DIR, x))\n",
        "\n",
        "# Create a numerical label: 0 for 'real', 1 for 'fake'\n",
        "df['label'] = df['status'].apply(lambda x: 0 if x == 'real' else 1)\n",
        "\n",
        "# --- Split the Data ---\n",
        "# Split into training (80%) and validation (20%) sets\n",
        "# stratify=df['label'] ensures both sets have a similar ratio of real/fake images\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "# Reset index for easier access later\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nTotal samples: {len(df)}\")\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Validation samples: {len(val_df)}\")\n",
        "print(\"\\nSample of the training dataframe:\")\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsKPO9X83mhR"
      },
      "source": [
        "### Step 3: Create a Custom PyTorch Dataset\n",
        "\n",
        "This class is essential for loading data efficiently. For each item, it:\n",
        "1. Reads an image from its file path.\n",
        "2. Converts it to RGB format to ensure consistency.\n",
        "3. Uses the SwinV2 `processor` to resize, normalize, and convert the image into a PyTorch tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVA_aaH044Ss",
        "outputId": "c7081eed-331f-4ac9-d60d-12dfa2ac6e96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset objects created successfully.\n",
            "Data augmentation pipeline is active for the training dataset.\n"
          ]
        }
      ],
      "source": [
        "# [REPLACE THE ORIGINAL STEP 3 CELL WITH THIS ONE]\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor\n",
        "from torchvision import transforms # Import the transforms module\n",
        "\n",
        "# --- Model and Processor Setup ---\n",
        "MODEL_CHECKPOINT = \"microsoft/swinv2-small-patch4-window8-256\"\n",
        "processor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# --- Data Augmentation Pipeline ---\n",
        "# Define a set of transformations to apply to the training images.\n",
        "# These operations are applied randomly to each image as it's loaded.\n",
        "train_transforms = transforms.Compose(\n",
        "    [\n",
        "        # Resize the image and crop it randomly to the model's input size.\n",
        "        # This helps the model focus on different parts of the image.\n",
        "        transforms.RandomResizedCrop(processor.size[\"height\"]),\n",
        "\n",
        "        # Flip the image horizontally with a 50% probability.\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "        # Slightly change the brightness, contrast, and saturation.\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "\n",
        "        # Rotate the image by a small random angle.\n",
        "        transforms.RandomRotation(10),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --- Updated Custom Dataset Class ---\n",
        "class ImageClassificationDataset(Dataset):\n",
        "    # Add a 'transforms' argument to the constructor\n",
        "    def __init__(self, df, processor, transforms=None):\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.df.iloc[idx]['full_path']\n",
        "        label = self.df.iloc[idx]['label']\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: File not found at {img_path}. Returning next item.\")\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "        # â­ï¸ Apply augmentations here, IF they are provided (i.e., for the training set)\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        # After optional augmentation, apply the model's required preprocessing\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# --- Create the Dataset Objects ---\n",
        "# Apply the 'train_transforms' pipeline ONLY to the training dataset.\n",
        "train_dataset = ImageClassificationDataset(train_df, processor, transforms=train_transforms)\n",
        "\n",
        "# The validation dataset should NOT be augmented, so we pass transforms=None.\n",
        "val_dataset = ImageClassificationDataset(val_df, processor, transforms=None)\n",
        "\n",
        "print(\"âœ… Dataset objects created successfully.\")\n",
        "print(f\"Data augmentation pipeline is active for the training dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P-Ebg2dq3mhS"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "# from PIL import Image\n",
        "# from transformers import AutoImageProcessor\n",
        "\n",
        "# # --- Model and Processor Setup ---\n",
        "# MODEL_CHECKPOINT = \"microsoft/swinv2-small-patch4-window8-256\"\n",
        "# processor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "# # --- Custom Dataset Class ---\n",
        "# class ImageClassificationDataset(Dataset):\n",
        "#     def __init__(self, df, processor):\n",
        "#         self.df = df\n",
        "#         self.processor = processor\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # Get image path and label from the dataframe\n",
        "#         img_path = self.df.iloc[idx]['full_path']\n",
        "#         label = self.df.iloc[idx]['label']\n",
        "\n",
        "#         # Open image, convert to RGB (handles grayscale, etc.)\n",
        "#         try:\n",
        "#             image = Image.open(img_path).convert(\"RGB\")\n",
        "#         except FileNotFoundError:\n",
        "#             print(f\"Warning: File not found at {img_path}. Returning next item.\")\n",
        "#             # A simple way to handle missing files is to load the next one\n",
        "#             return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "#         # Apply transformations (resizing, normalization)\n",
        "#         # return_tensors=\"pt\" gives us PyTorch tensors\n",
        "#         inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "#         # The processor output is batched, so we squeeze it to remove the batch dimension\n",
        "#         pixel_values = inputs['pixel_values'].squeeze(0)\n",
        "\n",
        "#         return {\n",
        "#             \"pixel_values\": pixel_values,\n",
        "#             \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "#         }\n",
        "\n",
        "# # Create the dataset objects for training and validation\n",
        "# train_dataset = ImageClassificationDataset(train_df, processor)\n",
        "# val_dataset = ImageClassificationDataset(val_df, processor)\n",
        "\n",
        "# print(\"\\nDataset objects created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bRs0hmo3mhS"
      },
      "source": [
        "### Step 4: Configure the Model and Training Arguments\n",
        "\n",
        "We load the pre-trained SwinV2 model and modify its classification head for our binary task (`num_labels=2`). Then, we define the training hyperparameters using `TrainingArguments`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a69oOQz3mhS",
        "outputId": "2e038e05-3a32-4bfe-cca7-1227a7c900f6"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# --- Load the Model ---\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# We specify num_labels=2 for our binary task (real/fake)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# ignore_mismatched_sizes=True allows us to replace the pre-trained classification head\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForImageClassification\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[0;32m   2316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2317\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2318\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2319\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2344\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2345\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\importlib\\__init__.py:88\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     87\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:42\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Optional, Union\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# ruff: isort: off\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     get_reporting_integration_callbacks,\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# ruff: isort: on\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhf_hub_utils\u001b[39;00m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[0;32m   2316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2317\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2318\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2319\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2344\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2345\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\importlib\\__init__.py:88\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     87\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:60\u001b[0m\n\u001b[0;32m     57\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TFPreTrainedModel\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
            "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[0;32m   2316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2317\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   2318\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   2319\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 2347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2344\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2345\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   2346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\importlib\\__init__.py:88\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     87\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n",
            "File \u001b[1;32mc:\\Users\\komal\\anaconda3\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_gelu\u001b[39m(x):\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    https://huggingface.co/papers/1606.08415\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# --- Load the Model ---\n",
        "# We specify num_labels=2 for our binary task (real/fake)\n",
        "# ignore_mismatched_sizes=True allows us to replace the pre-trained classification head\n",
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "MODEL_CHECKPOINT = \"microsoft/swinv2-small-patch4-window8-256\"\n",
        "\n",
        "# This line automatically finds and uses the correct class (SwinV2ForImageClassification)\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    MODEL_CHECKPOINT,\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "# --- Evaluation Metric ---\n",
        "# We'll use accuracy to monitor performance during training\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# --- Training Arguments ---\n",
        "# This object contains all the hyperparameters for training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./swinv2-real-vs-fake1\",\n",
        "    per_device_train_batch_size=8,  # Lower if you run out of GPU memory (e.g., 8)\n",
        "    per_device_eval_batch_size=16,\n",
        "    eval_strategy=\"epoch\",     # Evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\",           # Save a checkpoint at the end of each epoch\n",
        "    num_train_epochs=25,              # Start with a few epochs; you can increase this later\n",
        "    fp16=True,                       # Use mixed-precision training for speed (requires a T4/V100/A100 GPU)\n",
        "    learning_rate=2e-5,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=False,               # Set to True if you want to upload to Hugging Face Hub\n",
        "    remove_unused_columns=False,     # Important for our custom dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls1fFbzj3mhT"
      },
      "source": [
        "### Step 5: Start Training!\n",
        "\n",
        "With everything set up, we instantiate the `Trainer` and call the `.train()` method. This will kick off the fine-tuning process. Colab will show a progress bar with the loss and, at the end of each epoch, the validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "sX45vHEK3mhT",
        "outputId": "a723a343-e2fe-438b-e6b5-065aced961e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\komal\\AppData\\Local\\Temp\\ipykernel_33672\\3606930544.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5628' max='23450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5628/23450 1:22:56 < 4:22:44, 1.13 it/s, Epoch 6/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.235600</td>\n",
              "      <td>0.288476</td>\n",
              "      <td>0.889442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.218200</td>\n",
              "      <td>0.228412</td>\n",
              "      <td>0.931034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.129400</td>\n",
              "      <td>0.350319</td>\n",
              "      <td>0.916104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.126200</td>\n",
              "      <td>0.178832</td>\n",
              "      <td>0.956274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.115700</td>\n",
              "      <td>0.201478</td>\n",
              "      <td>0.951653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.123400</td>\n",
              "      <td>0.315777</td>\n",
              "      <td>0.939211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Training finished!\n"
          ]
        }
      ],
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "\n",
        "# --- Instantiate the Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=processor,             # The processor is passed here for data collating\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Add early stopping\n",
        ")\n",
        "\n",
        "# --- Start Training! ---\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "trainer.train()  # Change to a checkpoint path to resume training\n",
        "print(\"âœ… Training finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model(\"./best-model\")  # Save best model to a clean directory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fExL9t423mhT"
      },
      "source": [
        "### Step 6: Inference with the Fine-Tuned Model\n",
        "\n",
        "After training, the best model is saved. Let's load it and use it to predict whether a new, unseen image is real or fake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "y8X9rFDu3mhT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading best model from: ./swinv2-real-vs-fake\\checkpoint-8448\n",
            "\n",
            "Prediction for '00c461e5-9c5b-498c-937f-41bfc338565f.jpg':\n",
            "--> Predicted class: fake\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# The trainer saves the best model in the output directory specified in TrainingArguments\n",
        "# The `load_best_model_at_end=True` argument ensures this is the one with the highest accuracy\n",
        "best_model_path = trainer.state.best_model_checkpoint\n",
        "print(f\"Loading best model from: {best_model_path}\")\n",
        "\n",
        "# Load the fine-tuned model and processor\n",
        "inference_model = AutoModelForImageClassification.from_pretrained(best_model_path)\n",
        "inference_processor = AutoImageProcessor.from_pretrained(best_model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inference_model.to(device)\n",
        "\n",
        "# --- Make a prediction ---\n",
        "# â¬‡ï¸ IMPORTANT: Change this path to an image you want to test\n",
        "image_path = \"data/fake_SD/00c461e5-9c5b-498c-937f-41bfc338565f.jpg\" # Example path\n",
        "\n",
        "try:\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Process the image and move it to the GPU\n",
        "    inputs = inference_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Get model outputs (logits)\n",
        "    with torch.no_grad():\n",
        "        outputs = inference_model(**inputs)\n",
        "\n",
        "    # Get the predicted class\n",
        "    predicted_class_idx = outputs.logits.argmax(-1).item()\n",
        "    labels_map = ['real', 'fake'] # Corresponds to 0 and 1\n",
        "    print(f\"\\nPrediction for '{os.path.basename(image_path)}':\")\n",
        "    print(f\"--> Predicted class: {labels_map[predicted_class_idx]}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: The image file was not found at '{image_path}'.\")\n",
        "    print(\"Please update the 'image_path' variable to a valid file path in your Google Drive.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
