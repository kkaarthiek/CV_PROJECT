{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f9d8bf",
   "metadata": {},
   "source": [
    "# SwinV2 Small â€” Multiclass by Platform (Fake Images)\n",
    "\n",
    "This notebook fine-tunes a SwinV2 classifier to categorize fake images by their 'platform' in metadata.json.\n",
    "- It filters to status=='fake' and uses the 'platform' field as the class label.\n",
    "- If you want to include real images as an extra class, see notes in the data cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38899d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SEM5\\cv_project\\cv_train\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForImageClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9815d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.0+cu126 cuda available: True\n",
      "transformers: 4.57.1\n"
     ]
    }
   ],
   "source": [
    "# Quick environment check\n",
    "import transformers\n",
    "print('torch:', torch.__version__, 'cuda available:', torch.cuda.is_available())\n",
    "print('transformers:', transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0508f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from: ./data\\metadata.json\n",
      "Classes (platforms): ['None', 'dall-E3', 'im', 'sd']\n",
      "Total: 14062 Train: 11249 Val: 2813\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_file</th>\n",
       "      <th>prompts</th>\n",
       "      <th>platform</th>\n",
       "      <th>status</th>\n",
       "      <th>full_path</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1389</td>\n",
       "      <td>fake_SD/image_214.jpg</td>\n",
       "      <td>A woman in costume is marching with a large d...</td>\n",
       "      <td>sd</td>\n",
       "      <td>fake</td>\n",
       "      <td>./data\\fake_SD/image_214.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>791</td>\n",
       "      <td>fake_SD/6219a249-4462-4f76-ac23-37621c17751b.jpg</td>\n",
       "      <td>A shrub that has been shaped to look like a dog.</td>\n",
       "      <td>sd</td>\n",
       "      <td>fake</td>\n",
       "      <td>./data\\fake_SD/6219a249-4462-4f76-ac23-37621c1...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200</td>\n",
       "      <td>fake_SD/image_25.jpg</td>\n",
       "      <td>A older bearded man wearing a sports jacket m...</td>\n",
       "      <td>sd</td>\n",
       "      <td>fake</td>\n",
       "      <td>./data\\fake_SD/image_25.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8971</td>\n",
       "      <td>real/SD_dataset_000000252659.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>None</td>\n",
       "      <td>real</td>\n",
       "      <td>./data\\real/SD_dataset_000000252659.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12412</td>\n",
       "      <td>real/hf_unsplash_24713.jpg</td>\n",
       "      <td>real</td>\n",
       "      <td>None</td>\n",
       "      <td>real</td>\n",
       "      <td>./data\\real/hf_unsplash_24713.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                        image_file  \\\n",
       "0   1389                             fake_SD/image_214.jpg   \n",
       "1    791  fake_SD/6219a249-4462-4f76-ac23-37621c17751b.jpg   \n",
       "2   1200                              fake_SD/image_25.jpg   \n",
       "3   8971                  real/SD_dataset_000000252659.jpg   \n",
       "4  12412                        real/hf_unsplash_24713.jpg   \n",
       "\n",
       "                                             prompts platform status  \\\n",
       "0   A woman in costume is marching with a large d...       sd   fake   \n",
       "1   A shrub that has been shaped to look like a dog.       sd   fake   \n",
       "2   A older bearded man wearing a sports jacket m...       sd   fake   \n",
       "3                                               real     None   real   \n",
       "4                                               real     None   real   \n",
       "\n",
       "                                           full_path  label_id  \n",
       "0                       ./data\\fake_SD/image_214.jpg         3  \n",
       "1  ./data\\fake_SD/6219a249-4462-4f76-ac23-37621c1...         3  \n",
       "2                        ./data\\fake_SD/image_25.jpg         3  \n",
       "3            ./data\\real/SD_dataset_000000252659.jpg         0  \n",
       "4                  ./data\\real/hf_unsplash_24713.jpg         0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load and prepare metadata ---\n",
    "BASE_DIR = './data'\n",
    "METADATA_FILE = os.path.join(BASE_DIR, 'metadata.json')\n",
    "MODEL_CHECKPOINT = 'microsoft/swinv2-small-patch4-window8-256'\n",
    "\n",
    "print(f'Loading metadata from: {METADATA_FILE}')\n",
    "df = pd.read_json(METADATA_FILE)\n",
    "\n",
    "# Flatten image_file if nested lists are present\n",
    "if isinstance(df['image_file'].iloc[0], list):\n",
    "    df['image_file'] = df['image_file'].str[0]\n",
    "\n",
    "# Build absolute/full image paths\n",
    "df['full_path'] = df['image_file'].apply(lambda x: os.path.join(BASE_DIR, x))\n",
    "\n",
    "\n",
    "df = df[~df['platform'].isna()].copy()\n",
    "\n",
    "# Build label vocab from platform values\n",
    "label_names = sorted(df['platform'].unique().tolist())\n",
    "label2id = {l: i for i, l in enumerate(label_names)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "df['label_id'] = df['platform'].map(label2id).astype(int)\n",
    "\n",
    "# Split the data stratified by label\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df['label_id']\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "print('Classes (platforms):', label_names)\n",
    "print('Total:', len(df), 'Train:', len(train_df), 'Val:', len(val_df))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "982751b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets -> train: 11249 val: 2813\n"
     ]
    }
   ],
   "source": [
    "# --- Processor and Dataset ---\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
    "size = processor.size\n",
    "if isinstance(size, dict):\n",
    "    target = size.get('height') or size.get('shortest_edge') or 256\n",
    "else:\n",
    "    target = int(size) if size is not None else 256\n",
    "\n",
    "train_augs = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(target),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
    "    transforms.RandomRotation(10),\n",
    "])\n",
    "\n",
    "class PlatformDataset(Dataset):\n",
    "    def __init__(self, df, processor, transforms=None):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row['full_path']\n",
    "        label_id = int(row['label_id'])\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        img = self.transforms(image) if self.transforms is not None else image\n",
    "        inputs = self.processor(images=img, return_tensors='pt')\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "        return {'pixel_values': pixel_values, 'labels': torch.tensor(label_id, dtype=torch.long)}\n",
    "\n",
    "train_dataset = PlatformDataset(train_df, processor, transforms=train_augs)\n",
    "val_dataset = PlatformDataset(val_df, processor, transforms=None)\n",
    "print('Datasets -> train:', len(train_dataset), 'val:', len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3461a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Metrics ---\n",
    "metric = evaluate.load('accuracy')\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ce96bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TrainingArguments ---\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./swinv2-multiclass-realtrue',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=12,\n",
    "    fp16=use_fp16,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    remove_unused_columns=False,\n",
    "    report_to='none',\n",
    "    dataloader_pin_memory=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=0,\n",
    "    disable_tqdm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e39337c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Swinv2ForImageClassification were not initialized from the model checkpoint at microsoft/swinv2-small-patch4-window8-256 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\komal\\AppData\\Local\\Temp\\ipykernel_29940\\3656746040.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# --- Model + Trainer ---\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=len(label_names),\n",
    "    ignore_mismatched_sizes=True,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b9621a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {0: 'None', 1: 'dall-E3', 2: 'im', 3: 'sd'}\n",
      "ðŸš€ Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9849' max='16884' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9849/16884 1:41:52 < 1:12:46, 1.61 it/s, Epoch 7/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.536200</td>\n",
       "      <td>0.689419</td>\n",
       "      <td>0.734092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.449300</td>\n",
       "      <td>0.674576</td>\n",
       "      <td>0.808390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.281600</td>\n",
       "      <td>0.489135</td>\n",
       "      <td>0.869534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.287400</td>\n",
       "      <td>0.544420</td>\n",
       "      <td>0.873800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.404700</td>\n",
       "      <td>0.470495</td>\n",
       "      <td>0.902595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>1.244850</td>\n",
       "      <td>0.795592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.211200</td>\n",
       "      <td>0.780988</td>\n",
       "      <td>0.860647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training finished!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 01:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: {'eval_loss': 0.470495343208313, 'eval_accuracy': 0.9025950942054746, 'eval_runtime': 87.5298, 'eval_samples_per_second': 32.138, 'eval_steps_per_second': 2.011, 'epoch': 7.0}\n",
      "Best checkpoint: ./swinv2-multiclass-realtrue\\checkpoint-7035\n",
      "Saved model to ./best-model-swinv2-multiclass-realtrue\n",
      "Saved model to ./best-model-swinv2-multiclass-realtrue\n"
     ]
    }
   ],
   "source": [
    "print('Classes:', id2label)\n",
    "print('ðŸš€ Starting training...')\n",
    "_ = trainer.train()\n",
    "print('âœ… Training finished!')\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print('Validation metrics:', metrics)\n",
    "\n",
    "best_ckpt = getattr(trainer.state, 'best_model_checkpoint', None)\n",
    "save_dir = './best-model-swinv2-multiclass-realtrue'\n",
    "if best_ckpt:\n",
    "    print('Best checkpoint:', best_ckpt)\n",
    "trainer.save_model(save_dir)\n",
    "print('Saved model to', save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81aa1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
